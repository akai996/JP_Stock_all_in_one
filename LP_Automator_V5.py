import os
import random
import string
import json
import shutil
import requests
import datetime
import urllib.parse
from bs4 import BeautifulSoup

# ================= é…ç½®åŒº =================
TARGET_REAL_PAGE = "index.html"  
OUTPUT_ZIP_NAME = "upload_me_v5_fixed"     
XOR_KEY = random.randint(10, 250) 
# ==========================================

class LPAutomatorV5:
    def __init__(self):
        self.dist_dir = "dist_lp"
        self.white_file = "white_template.html"
        self.map = {}
        if os.path.exists(self.dist_dir): shutil.rmtree(self.dist_dir)
        os.makedirs(self.dist_dir, exist_ok=True)

    def _rand_str(self, length=8):
        return ''.join(random.choices(string.ascii_lowercase, k=length))

    def fetch_news_and_gen_white(self):
        """æ­¥éª¤ 1: å®æ—¶é‡‡é›†æ—¥ç»æ–°é—»å¹¶ç”Ÿæˆé•¿ç¯‡ç™½é¡µ"""
        print("ğŸ“¡ æ­£åœ¨é‡‡é›†æœ€æ–°è´¢ç»èµ„è®¯...")
        url = "https://www.nikkei.com/news/category/market/"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        try:
            res = requests.get(url, headers=headers, timeout=15)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = []
            # å¢åŠ é‡‡é›†æ•°é‡è‡³15æ¡ï¼Œç¡®ä¿è¶³å¤Ÿæ»šåŠ¨é«˜åº¦
            for item in soup.select('article')[:15]:
                title = item.find('span', class_=lambda x: x and 'title' in x)
                summary = item.find('p')
                if title:
                    articles.append({"t": title.get_text().strip(), "s": summary.get_text().strip() if summary else "å¸‚å ´ã®å‹•å‘ã«é–¢ã™ã‚‹è©³ç´°ãªåˆ†æãŒé€²è¡Œä¸­ã§ã™..."})
            
            if not articles: raise ValueError("æœªèƒ½æå–åˆ°æœ‰æ•ˆæ–°é—»å†…å®¹")

            html = f"""<!DOCTYPE html><html lang="ja"><head><meta charset="UTF-8"><title>Market Insight Japan</title>
            <style>body{{font-family:sans-serif;color:#333;line-height:1.8;padding:20px;background:#f4f4f4}}
            .c{{max-width:800px;margin:auto;background:#fff;padding:40px;box-shadow:0 0 10px rgba(0,0,0,0.1)}}
            .a{{margin-bottom:30px;border-bottom:1px solid #eee;padding-bottom:20px}}
            .t{{font-size:20px;color:#003366;font-weight:bold}}</style></head><body>
            <div class="c"><h2>ãƒãƒ¼ã‚±ãƒƒãƒˆé€Ÿå ± ({datetime.date.today()})</h2>"""
            for a in articles:
                html += f"<div class='a'><div class='t'>{a['t']}</div><p>{a['s']}</p></div>"
            html += f"<div style='text-align:center;color:#999;font-size:12px'>Â© {datetime.date.today().year} Market Insight Japan</div></div></body></html>"
            
            with open(self.white_file, "w", encoding="utf-8") as f: f.write(html)
            print("âœ… ç™½é¡µæ¨¡æ¿å·²åŒæ­¥æ›´æ–°ã€‚")
        except Exception as e:
            print(f"âš ï¸ é‡‡é›†å¤±è´¥: {e}ï¼Œæ­£åœ¨ç”Ÿæˆå¤‡ç”¨æœ¬åœ°æ¨¡æ¿...")
            # å¤‡ç”¨æœ¬åœ°é™æ€æ¨¡æ¿é€»è¾‘
            with open(self.white_file, "w", encoding="utf-8") as f: f.write("<html><body>æœ¬åœ°é™æ€ç™½é¡µå†…å®¹</body></html>")

    def scramble_and_pack(self):
        """æ­¥éª¤ 2: æ‰§è¡Œ V5 çº§å¤šæ€æ··æ·†"""
        print("ğŸ” æ­£åœ¨æ‰§è¡Œ V5 çº§é€»è¾‘æ··æ·†...")
        if not os.path.exists(self.white_file): return

        with open(self.white_file, 'r', encoding='utf-8') as f:
            w_soup = BeautifulSoup(f.read(), 'html.parser')
            w_body = "".join([str(x) for x in w_soup.body.contents]) if w_soup.body else str(w_soup)
            w_title = w_soup.title.string if w_soup.title else "Market News"

        with open(TARGET_REAL_PAGE, 'r', encoding='utf-8') as f:
            r_soup = BeautifulSoup(f.read(), 'html.parser')
            
            # ä¿®å¤ï¼šç´ æè·¯å¾„å‡€åŒ–é€»è¾‘
            for tag, attr in {'img':'src', 'link':'href', 'script':'src'}.items():
                for el in r_soup.find_all(tag):
                    src = el.get(attr)
                    if src and not src.startswith(('http', '//', 'data:')):
                        # å»é™¤ URL å‚æ•°å¦‚ ?v=1
                        clean_src = urllib.parse.urlparse(src).path
                        dest = os.path.join(self.dist_dir, clean_src)
                        os.makedirs(os.path.dirname(dest), exist_ok=True)
                        if os.path.exists(clean_src): 
                            shutil.copy(clean_src, dest)

        # æ ¸å¿ƒå†…å®¹ XOR åŠ å¯† (ä¿®æ­£ç¼–ç é—®é¢˜)
        # å»ºè®®åœ¨ index.html ä¸­ä½¿ç”¨ id="main-content" åŒ…è£¹æ•æ„Ÿå†…å®¹
        target_node = r_soup.find(id="main-content") or r_soup.body
        if not target_node:
            print("âŒ é”™è¯¯ï¼šindex.html ç»“æ„ä¸å®Œæ•´ï¼Œæ‰¾ä¸åˆ° bodyã€‚")
            return

        raw_html = "".join([str(x) for x in target_node.contents])
        encoded = [ord(c) ^ XOR_KEY for c in raw_html]
        
        # éšæœºåŒ–è§£å¯†é€»è¾‘å˜é‡å
        v_data, v_key, v_res, v_check, v_root = [self._rand_str(6) for _ in range(5)]

        final_html = f"""<!DOCTYPE html><html><head><meta charset="UTF-8">
        <meta name="viewport" content="width=device-width,initial-scale=1.0"><title>{w_title}</title>
        <style>body{{margin:0;padding:0}}#{v_root}{{min-height:210vh;background:#fff}}</style></head>
        <body><div id="{v_root}">{w_body}</div><script>
        (function(){{
            var {v_data}={json.dumps(encoded)}, {v_key}={XOR_KEY}, _r=false, _t=false;
            function _ex(){{
                if(_r||navigator.webdriver||document.visibilityState!=='visible')return;
                _r=true; try{{
                    var {v_res}={v_data}.map(function(c){{return String.fromCharCode(c^{v_key})}}).join('');
                    document.body.innerHTML={v_res};window.scrollTo(0,0);
                }}catch(e){{console.clear();}}
            }}
            function {v_check}(){{ if(!_t&&window.scrollY>500){{_t=true;setTimeout(_ex,3200);}} }}
            window.addEventListener('scroll',{v_check});window.addEventListener('touchmove',{v_check});
        }})();</script></body></html>"""

        with open(os.path.join(self.dist_dir, "index.html"), "w", encoding="utf-8") as f: 
            f.write(final_html)

    def create_zip(self):
        """æ­¥éª¤ 3: å‹ç¼©æ‰“åŒ…"""
        try:
            print(f"ğŸ“¦ æ­£åœ¨æ‰“åŒ…äº§ç‰©ä¸º {OUTPUT_ZIP_NAME}.zip...")
            shutil.make_archive(OUTPUT_ZIP_NAME, 'zip', self.dist_dir)
            print(f"âœ¨ æµç¨‹ç»“æŸï¼ZIPæ–‡ä»¶å·²ç”Ÿæˆåœ¨å½“å‰ç›®å½•ã€‚")
        except Exception as e:
            print(f"âŒ æ‰“åŒ…å¤±è´¥: {e}")

if __name__ == "__main__":
    if not os.path.exists(TARGET_REAL_PAGE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {TARGET_REAL_PAGE}")
    else:
        flow = LPAutomatorV5()
        flow.fetch_news_and_gen_white()
        flow.scramble_and_pack()
        flow.create_zip()
        input("\nä»»åŠ¡ç»“æŸï¼ŒæŒ‰å›è½¦é€€å‡º...")
