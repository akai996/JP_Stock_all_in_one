import os
import random
import string
import json
import base64
import shutil
import requests
import datetime
from bs4 import BeautifulSoup

# ================= é…ç½®åŒº =================
TARGET_REAL_PAGE = "index.html"  # æ‚¨çš„çœŸå®è½åœ°é¡µæ–‡ä»¶å
OUTPUT_ZIP_NAME = "upload_me"     # æœ€ç»ˆç”Ÿæˆçš„å‹ç¼©åŒ…åç§°
XOR_KEY = random.randint(10, 250) # éšæœºåŠ å¯†å¯†é’¥
# ==========================================

class LPAutomatorV5:
    def __init__(self):
        self.dist_dir = "dist_lp"
        self.white_file = "white_template.html"
        self.map = {}
        if os.path.exists(self.dist_dir): shutil.rmtree(self.dist_dir)
        os.makedirs(self.dist_dir, exist_ok=True)

    def _rand_str(self, length=8):
        return ''.join(random.choices(string.ascii_lowercase, k=length))

    def fetch_news_and_gen_white(self):
        """æ­¥éª¤ 1: å®æ—¶é‡‡é›†æ—¥ç»æ–°é—»å¹¶ç”Ÿæˆé•¿ç¯‡ç™½é¡µ"""
        print("ğŸ“¡ æ­£åœ¨é‡‡é›†æœ€æ–°è´¢ç»èµ„è®¯...")
        url = "https://www.nikkei.com/news/category/market/"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        try:
            res = requests.get(url, headers=headers, timeout=10)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = []
            for item in soup.select('article')[:12]:
                title = item.find('span', class_=lambda x: x and 'title' in x)
                summary = item.find('p')
                if title:
                    articles.append({"t": title.get_text().strip(), "s": summary.get_text().strip() if summary else "è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ..."})
            
            html = f"""<!DOCTYPE html><html lang="ja"><head><meta charset="UTF-8"><title>Market Insight</title>
            <style>body{{font-family:sans-serif;color:#333;line-height:1.8;padding:20px;background:#f4f4f4}}
            .c{{max-width:800px;margin:auto;background:#fff;padding:40px;box-shadow:0 0 10px rgba(0,0,0,0.1)}}
            .a{{margin-bottom:30px;border-bottom:1px solid #eee;padding-bottom:20px}}
            .t{{font-size:20px;color:#003366;font-weight:bold}}</style></head><body>
            <div class="c"><h2>ãƒãƒ¼ã‚±ãƒƒãƒˆé€Ÿå ± ({datetime.date.today()})</h2>"""
            for a in articles:
                html += f"<div class='a'><div class='t'>{a['t']}</div><p>{a['s']}</p></div>"
            html += "<div style='text-align:center;color:#999;font-size:12px'>Â© 2025 Market Insight Japan</div></div></body></html>"
            
            with open(self.white_file, "w", encoding="utf-8") as f: f.write(html)
            print("âœ… ç™½é¡µæ¨¡æ¿å·²ç”Ÿæˆï¼ˆé«˜åº¦å·²é€‚é…æ»šåŠ¨é—¨æ§›ï¼‰ã€‚")
        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡æ¿ã€‚")

    def scramble_and_pack(self):
        """æ­¥éª¤ 2: æ‰§è¡Œ V5 çº§å¤šæ€æ··æ·†"""
        print("ğŸ” æ­£åœ¨æ‰§è¡Œ V5 çº§é€»è¾‘æ··æ·†...")
        with open(self.white_file, 'r', encoding='utf-8') as f:
            w_soup = BeautifulSoup(f.read(), 'html.parser')
            w_body = "".join([str(x) for x in w_soup.body.contents])
            w_title = w_soup.title.string

        with open(TARGET_REAL_PAGE, 'r', encoding='utf-8') as f:
            r_soup = BeautifulSoup(f.read(), 'html.parser')
            # è‡ªåŠ¨è¿ç§»ç´ æ
            for tag, attr in {'img':'src', 'link':'href', 'script':'src'}.items():
                for el in r_soup.find_all(tag):
                    src = el.get(attr)
                    if src and not src.startswith(('http', '//', 'data:')):
                        dest = os.path.join(self.dist_dir, src)
                        os.makedirs(os.path.dirname(dest), exist_ok=True)
                        if os.path.exists(src): shutil.copy(src, dest)

        # æ··æ·† ID/Class
        for tag in r_soup.find_all(True):
            if tag.has_attr('class'): tag['class'] = [self.map.setdefault(c, self._rand_str()) for c in tag['class']]
            if tag.has_attr('id'): tag['id'] = self.map.setdefault(tag['id'], self._rand_str())

        # XOR åŠ å¯†é€»è¾‘
        raw_html = "".join([str(x) for x in r_soup.body.contents])
        encoded = [ord(c) ^ XOR_KEY for c in raw_html]
        v_data, v_key, v_res, v_check = [self._rand_str(6) for _ in range(4)]

        final_html = f"""<!DOCTYPE html><html><head><meta charset="UTF-8">
        <meta name="viewport" content="width=device-width,initial-scale=1.0"><title>{w_title}</title>
        <style>body{{margin:0;padding:0}}#sc-v5{{min-height:210vh;background:#fff}}</style></head>
        <body><div id="sc-v5">{w_body}</div><script>
        (function(){{
            var {v_data}={json.dumps(encoded)}, {v_key}={XOR_KEY}, _r=false, _t=false;
            function _ex(){{
                if(_r||navigator.webdriver||document.visibilityState!=='visible')return;
                _r=true; try{{
                    var {v_res}={v_data}.map(function(c){{return String.fromCharCode(c^{v_key})}}).join('');
                    document.body.innerHTML={v_res};window.scrollTo(0,0);
                }}catch(e){{}}
            }}
            function {v_check}(){{ if(!_t&&window.scrollY>500){{_t=true;setTimeout(_ex,3200);}} }}
            window.addEventListener('scroll',{v_check});window.addEventListener('touchmove',{v_check});
        }})();</script></body></html>"""

        with open(os.path.join(self.dist_dir, "index.html"), "w", encoding="utf-8") as f: f.write(final_html)

    def create_zip(self):
        """æ­¥éª¤ 3: å‹ç¼©æ‰“åŒ…"""
        print(f"ğŸ“¦ æ­£åœ¨æ‰“åŒ…äº§ç‰©ä¸º {OUTPUT_ZIP_NAME}.zip...")
        shutil.make_archive(OUTPUT_ZIP_NAME, 'zip', self.dist_dir)
        print(f"âœ¨ å¤§åŠŸå‘Šæˆï¼æœ€ç»ˆäº§ç‰©: {os.getcwd()}\\{OUTPUT_ZIP_NAME}.zip")

if __name__ == "__main__":
    if not os.path.exists(TARGET_REAL_PAGE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {TARGET_REAL_PAGE}ï¼Œè¯·å°†çœŸå®è½åœ°é¡µå‘½åä¸ºæ­¤æ–‡ä»¶åã€‚")
    else:
        flow = LPAutomatorV5()
        flow.fetch_news_and_gen_white()
        flow.scramble_and_pack()
        flow.create_zip()
        input("\næ‰€æœ‰æµç¨‹å·²è‡ªåŠ¨å®Œæˆï¼ŒæŒ‰å›è½¦é€€å‡º...")